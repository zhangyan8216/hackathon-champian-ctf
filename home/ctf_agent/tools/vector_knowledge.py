#!/usr/bin/env python3
"""
å‘é‡åŒ–çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ - å¢å¼ºçŸ¥è¯†åº“ç®¡ç†

åŠŸèƒ½ï¼š
- æ–‡æœ¬å‘é‡åŒ–ï¼ˆä½¿ç”¨å¤šç§Embeddingæ¨¡å‹ï¼‰
- è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
- æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰
- çŸ¥è¯†èšç±»å’Œå»é‡
- è‡ªåŠ¨çŸ¥è¯†æ›´æ–°
- ç¦»çº¿ç´¢å¼•æ„å»º
- å¢é‡ç´¢å¼•æ›´æ–°
- ç›¸å…³æ€§è¯„åˆ†
"""

import json
import hashlib
import pickle
import time
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict
import re


class TextVectorizer:
    """æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, method: str = 'tfidf'):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        
        Args:
            method: å‘é‡åŒ–æ–¹æ³• ('tfidf', 'hash', 'co-occurrence', 'keyword')
        """
        self.method = method
        self.vocabulary = {}
        self.idf = {}
        self.document_count = 0
    
    def train(self, documents: List[str]):
        """
        è®­ç»ƒå‘é‡åŒ–å™¨
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        self.document_count = len(documents)
        
        # æ„å»ºè¯è¡¨
        word_doc_count = defaultdict(int)
        
        for doc in documents:
            words = self._tokenize(doc)
            unique_words = set(words)
            
            for word in unique_words:
                self.vocabulary[word] = self.vocabulary.get(word, 0) + word_doc_count[word] + 1
                word_doc_count[word] += 1
        
        # è®¡ç®—IDF
        for word, doc_count in word_doc_count.items():
            self.idf[word] = self._calculate_idf(doc_count, self.document_count)
    
    def vectorize(self, text: str) -> Dict[str, float]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            å‘é‡å­—å…¸ {word: weight}
        """
        if self.method == 'tfidf':
            return self._tfidf_vectorize(text)
        elif self.method == 'hash':
            return self._hash_vectorize(text)
        elif self.method == 'co-occurrence':
            return self._cooccurrence_vectorize(text)
        else:  # keyword
            return self._keyword_vectorize(text)
    
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        # å°å†™åŒ–
        text = text.lower()
        
        # æå–å•è¯
        words = re.findall(r'\b\w+\b', text)
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',
            'can', 'could', 'may', 'might', 'must', 'shall', 'this', 'that', 'these',
            'those', 'a', 'an', 'it', 'its', 'they', 'them', 'their', 'you', 'your'
        }
        
        words = [w for w in words if w not in stop_words and len(w) > 2]
        
        # è¯å¹²æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        words = [self._stem(w) for w in words]
        
        return words
    
    def _stem(self, word: str) -> str:
        """ç®€åŒ–è¯å¹²æå–"""
        # ç®€å•è§„åˆ™ï¼šå»é™¤å¸¸è§åç¼€
        suffixes = ['ing', 'ly', 'ed', 'ies', 'es', 's']
        
        for suffix in suffixes:
            if word.endswith(suffix) and len(word) > len(suffix) + 2:
                return word[:-len(suffix)]
        
        return word
    
    def _calculate_tf(self, word: str, document: str) -> float:
        """è®¡ç®—è¯é¢‘ï¼ˆTFï¼‰"""
        words = self._tokenize(document)
        word_count = words.count(word)
        return word_count / len(words)
    
    def _calculate_idf(self, doc_count: int, total_docs: int) -> float:
        """è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰"""
        import math
        return math.log(total_docs / (doc_count + 1)) + 1
    
    def _tfidf_vectorize(self, text: str) -> Dict[str, float]:
        """TF-IDFå‘é‡åŒ–"""
        words = self._tokenize(text)
        vector = {}
        
        # è®¡ç®—æ¯ä¸ªè¯çš„TF-IDF
        unique_words = set(words)
        
        for word in unique_words:
            tf = self._calculate_tf(word, text)
            idf = self.idf.get(word, 1.0)
            tfidf = tf * idf
            
            vector[word] = tfidf
        
        return vector
    
    def _hash_vectorize(self, text: str, n_buckets: int = 1024) -> Dict[str, float]:
        """Hashå‘é‡åŒ–ï¼ˆç®€åŒ–ç‰ˆSimHashï¼‰"""
        words = self._tokenize(text)
        vector = {}
        
        for word in words:
            # è®¡ç®—wordçš„hash
            hash_value = int(hashlib.md5(word.encode()).hexdigest(), 16)
            
            # æ˜ å°„åˆ°bucket
            bucket = hash_value % n_buckets
            vector[f'bucket_{bucket}'] = vector.get(f'bucket_{bucket}', 0) + 1
        
        # å½’ä¸€åŒ–
        total = sum(vector.values())
        if total > 0:
            vector = {k: v / total for k, v in vector.items()}
        
        return vector
    
    def _cooccurrence_vectorize(self, text: str, window: int = 2) -> Dict[str, float]:
        """å…±ç°å‘é‡åŒ–"""
        words = self._tokenize(text)
        vector = {}
        
        # ç»Ÿè®¡å…±ç°è¯å¯¹
        for i in range(len(words)):
            for j in range(i + 1, min(i + window + 1, len(words))):
                word1 = words[i]
                word2 = words[j]
                
                pair = f'{word1}_{word2}'
                vector[pair] = vector.get(pair, 0) + 1
        
        # å½’ä¸€åŒ–
        total = sum(vector.values())
        if total > 0:
            vector = {k: v / total for k, v in vector.items()}
        
        return vector
    
    def _keyword_vectorize(self, text: str) -> Dict[str, float]:
        """å…³é”®è¯å‘é‡åŒ–ï¼ˆTFç®€åŒ–ç‰ˆï¼‰"""
        words = self._tokenize(text)
        vector = {}
        
        # è¯é¢‘
        word_counts = defaultdict(int)
        for word in words:
            word_counts[word] += 1
        
        # æŒ‰è¯é¢‘æ’åºï¼Œå–å‰20ä¸ª
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # å½’ä¸€åŒ–æƒé‡
        total = sum(count for _, count in sorted_words)
        for word, count in sorted_words:
            vector[word] = count / total
        
        return vector


class SimilarityCalculator:
    """ç›¸ä¼¼åº¦è®¡ç®—å™¨"""
    
    @staticmethod
    def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # è·å–å…±åŒçš„keys
        common_keys = set(vec1.keys()) & set(vec2.keys())
        
        if not common_keys:
            return 0.0
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(vec1[k] * vec2[k] for k in common_keys)
        
        # è®¡ç®—å‘é‡é•¿åº¦
        norm1 = (sum(v ** 2 for v in vec1.values())) ** 0.5
        norm2 = (sum(v ** 2 for v in vec2.values())) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return max(0.0, min(1.0, similarity))
    
    @staticmethod
    def jaccard_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """
        è®¡ç®—Jaccardç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        keys1 = set(vec1.keys())
        keys2 = set(vec2.keys())
        
        intersection = keys1 & keys2
        union = keys1 | keys2
        
        if not union:
            return 0.0
        
        return len(intersection) / len(union)
    
    @staticmethod
    def euclidean_distance(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """
        è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
        
        Returns:
            è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
        """
        all_keys = set(vec1.keys()) | set(vec2.keys())
        
        distance = 0.0
        for key in all_keys:
            v1 = vec1.get(key, 0)
            v2 = vec2.get(key, 0)
            distance += (v2 - v1) ** 2
        
        return distance ** 0.5


class VectorKnowledgeBase:
    """å‘é‡åŒ–çŸ¥è¯†åº“"""
    
    def __init__(self, storage_path: str = None, vectorizer_method: str = 'tfidf'):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        
        Args:
            storage_path: å­˜å‚¨è·¯å¾„
            vectorizer_method: å‘é‡åŒ–æ–¹æ³•
        """
        self.storage_path = storage_path or 'memory/vector_kb'
        self.vectorizer = TextVectorizer(method=vectorizer_method)
        self.similarity_calc = SimilarityCalculator()
        
        # çŸ¥è¯†åº“
        self.documents = {}  # {doc_id: document}
        self.vectors = {}    # {doc_id: vector}
        self.metadata = {}   # {doc_id: metadata}
        
        # ç´¢å¼•
        self.inverse_index = defaultdict(list)  # {word: [doc_ids]}
        
        # ç»Ÿè®¡
        self.stats = {
            'total_docs': 0,
            'avg_vector_size': 0,
            'last_updated': time.time()
        }
        
        # åŠ è½½å·²æœ‰çŸ¥è¯†
        self._load()
    
    def _load(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            base_path = Path(self.storage_path)
            
            # åŠ è½½æ–‡æ¡£
            docs_path = base_path / 'documents.pkl'
            if docs_path.exists():
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)
            
            # åŠ è½½å‘é‡
            vectors_path = base_path / 'vectors.pkl'
            if vectors_path.exists():
                with open(vectors_path, 'rb') as f:
                    self.vectors = pickle.load(f)
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = base_path / 'metadata.pkl'
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_docs'] = len(self.documents)
            self.stats['last_updated'] = base_path.stat().st_mtime if base_path.exists() else time.time()
            
            print(f"  åŠ è½½äº† {len(self.documents)} ç¯‡æ–‡æ¡£")
            
        except Exception as e:
            print(f"  åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def _save(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        try:
            base_path = Path(self.storage_path)
            base_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ–‡æ¡£
            with open(base_path / 'documents.pkl', 'wb') as f:
                pickle.dump(self.documents, f)
            
            # ä¿å­˜å‘é‡
            with open(base_path / 'vectors.pkl', 'wb') as f:
                pickle.dump(self.vectors, f)
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(base_path / 'metadata.pkl', 'wb') as f:
                pickle.dump(self.metadata, f)
            
            self.stats['last_updated'] = time.time()
            
        except Exception as e:
            print(f"  ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def add_document(self, doc_id: str, text: str, metadata: Dict = None):
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            doc_id: æ–‡æ¡£ID
            text: æ–‡æ¡£æ–‡æœ¬
            metadata: å…ƒæ•°æ®
        """
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self._clean_text(text)
        
        # å‘é‡åŒ–
        vector = self.vectorizer.vectorize(cleaned_text)
        
        # å­˜å‚¨
        self.documents[doc_id] = cleaned_text
        self.vectors[doc_id] = vector
        self.metadata[doc_id] = metadata or {}
        
        # æ›´æ–°å€’æ’ç´¢å¼•
        words = self.vectorizer._tokenize(cleaned_text)
        for word in words:
            if doc_id not in self.inverse_index[word]:
                self.inverse_index[word].append(doc_id)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_docs'] += 1
        
        # å¢é‡æ›´æ–°IDFï¼ˆç®€åŒ–ç‰ˆï¼šé‡æ–°è®­ç»ƒï¼‰
        self._retrain_vectorizer()
    
    def _retrain_vectorizer(self):
        """é‡æ–°è®­ç»ƒå‘é‡åŒ–å™¨"""
        all_texts = list(self.documents.values())
        self.vectorizer.train(all_texts)
        
        # é‡æ–°å‘é‡åŒ–æ‰€æœ‰æ–‡æ¡£
        for doc_id, text in self.documents.items():
            self.vectors[doc_id] = self.vectorizer.vectorize(text)
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', ' ', text)
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^a-zA-Z0-9\s\.\,\!\?\-\_]', ' ', text)
        
        return text.strip()
    
    def search(self, query: str, top_k: int = 10, 
              similarity_threshold: float = 0.1) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vec = self.vectorizer.vectorize(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        results = []
        
        for doc_id, doc_vec in self.vectors.items():
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = self.similarity_calc.cosine_similarity(query_vec, doc_vec)
            
            if similarity >= similarity_threshold:
                results.append({
                    'doc_id': doc_id,
                    'similarity': similarity,
                    'text': self.documents[doc_id],
                    'metadata': self.metadata[doc_id]
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]
    
    def hybrid_search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # å…³é”®è¯æœç´¢
        keywords = self.vectorizer._tokenize(query)
        keyword_scores = defaultdict(float)
        
        for keyword in keywords:
            for doc_id in self.inverse_index.get(keyword, []):
                keyword_scores[doc_id] += 1
        
        # è¯­ä¹‰æœç´¢
        semantic_results = self.search(query, top_k=top_k * 2)
        
        # åˆå¹¶åˆ†æ•°
        combined_results = []
        
        for result in semantic_results:
            doc_id = result['doc_id']
            semantic_score = result['similarity']
            keyword_score = keyword_scores.get(doc_id, 0)
            
            # åŠ æƒç»„åˆï¼ˆ70%è¯­ä¹‰ + 30%å…³é”®è¯ï¼‰
            combined_score = semantic_score * 0.7 + min(keyword_score / 10, 0.3)
            
            combined_results.append({
                **result,
                'combined_score': combined_score,
                'keyword_score': keyword_score
            })
        
        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
        
        return combined_results[:top_k]
    
    def find_similar(self, doc_id: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ‰¾åˆ°ç›¸ä¼¼çš„æ–‡æ¡£
        
        Args:
            doc_id: å‚è€ƒæ–‡æ¡£ID
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        if doc_id not in self.vectors:
            return []
        
        query_vec = self.vectors[doc_id]
        
        results = []
        
        for other_id, other_vec in self.vectors.items():
            if other_id == doc_id:
                continue
            
            similarity = self.similarity_calc.cosine_similarity(query_vec, other_vec)
            
            results.append({
                'doc_id': other_id,
                'similarity': similarity,
                'text': self.documents[other_id],
                'metadata': self.metadata[other_id]
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]
    
    def cluster_documents(self, n_clusters: int = 5) -> Dict[str, List[str]]:
        """
        æ–‡æ¡£èšç±»ï¼ˆç®€åŒ–ç‰ˆK-Meansï¼‰
        
        Args:
            n_clusters: èšç±»æ•°é‡
        
        Returns:
            èšç±»ç»“æœ {cluster_id: [doc_ids]}
        """
        import random
        
        doc_ids = list(self.vectors.keys())
        if len(doc_ids) < n_clusters:
            return {f'cluster_{i}': [] for i in range(n_clusters)}
        
        # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        centers = []
        for _ in range(n_clusters):
            center_id = random.choice(doc_ids)
            centers.append(self.vectors[center_id])
        
        # ç®€åŒ–çš„K-Meansï¼ˆåªè¿›è¡Œ10æ¬¡è¿­ä»£ï¼‰
        clusters = defaultdict(list)
        
        for iteration in range(10):
            clusters.clear()
            
            # åˆ†é…æ–‡æ¡£åˆ°æœ€è¿‘çš„èšç±»
            for doc_id in doc_ids:
                doc_vec = self.vectors[doc_id]
                
                similarities = [
                    self.similarity_calc.cosine_similarity(doc_vec, center)
                    for center in centers
                ]
                
                best_cluster = similarities.index(max(similarities))
                clusters[f'cluster_{best_cluster}'].append(doc_id)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centers = []
            for cluster_id, cluster_docs in clusters.items():
                if not cluster_docs:
                    continue
                
                # è®¡ç®—å¹³å‡å‘é‡
                avg_vector = defaultdict(float)
                for doc_id in cluster_docs:
                    doc_vec = self.vectors[doc_id]
                    for word, weight in doc_vec.items():
                        avg_vector[word] += weight
                
                # å½’ä¸€åŒ–
                total = sum(avg_vector.values())
                avg_vector = {k: v / total for k, v in avg_vector.items()}
                new_centers.append(avg_vector)
            
            centers = new_centers
        
        return dict(clusters)
    
    def deduplicate(self, similarity_threshold: float = 0.95) -> List[str]:
        """
        å»é‡éå¸¸ç›¸ä¼¼çš„æ–‡æ¡£
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            é‡å¤çš„æ–‡æ¡£IDåˆ—è¡¨
        """
        duplicates = []
        checked = set()
        
        for doc_id1, vec1 in self.vectors.items():
            if doc_id1 in checked:
                continue
            
            for doc_id2, vec2 in self.vectors.items():
                if doc_id1 == doc_id2 or doc_id2 in checked:
                    continue
                
                similarity = self.similarity_calc.cosine_similarity(vec1, vec2)
                
                if similarity >= similarity_threshold:
                    duplicates.append(doc_id2)
                    checked.add(doc_id2)
            
            checked.add(doc_id1)
        
        return duplicates
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_vec_size = sum(len(v) for v in self.vectors.values()) / len(self.vectors) if self.vectors else 0
        
        return {
            'total_documents': len(self.documents),
            'total_vectors': len(self.vectors),
            'average_vector_size': avg_vec_size,
            'vocabulary_size': len(self.vectorizer.vocabulary),
            'last_updated': self.stats['last_updated']
        }
    
    def export(self, filepath: str):
        """å¯¼å‡ºçŸ¥è¯†åº“ä¸ºJSON"""
        export_data = {
            'documents': self.documents,
            'metadata': self.metadata,
            'stats': self.get_stats(),
            'export_time': time.time()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    def import_from_json(self, filepath: str):
        """ä»JSONå¯¼å…¥çŸ¥è¯†åº“"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for doc_id, text in data['documents'].items():
            self.add_document(doc_id, text, data['metadata'].get(doc_id))


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("ğŸ“Š å‘é‡åŒ–çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ\n")
    print("="*60)
    
    # åˆ›å»ºçŸ¥è¯†åº“
    kb = VectorKnowledgeBase(storage_path='memory/test_kb', vectorizer_method='tfidf')
    
    # æ·»åŠ æ–‡æ¡£
    documents = [
        ("doc1", "Base64ç¼–ç æ˜¯ä¸€ç§å¸¸è§çš„ç¼–ç æ–¹å¼ï¼Œç”¨äºå°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºASCIIå­—ç¬¦ã€‚", {"category": "crypto"}),
        ("doc2", "SQLæ³¨å…¥æ˜¯é€šè¿‡æ’å…¥æ¶æ„SQLè¯­å¥æ¥æ”»å‡»æ•°æ®åº“çš„æ¼æ´ã€‚", {"category": "web"}),
        ("doc3", "XSSè·¨ç«™è„šæœ¬æ”»å‡»æ˜¯é€šè¿‡æ³¨å…¥æ¶æ„è„šæœ¬æ¥æ”»å‡»ç”¨æˆ·çš„æ¼æ´ã€‚", {"category": "web"}),
        ("doc4", "Hexåå…­è¿›åˆ¶æ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°å­—è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äºè¡¨ç¤ºäºŒè¿›åˆ¶æ•°æ®ã€‚", {"category": "crypto"}),
        ("doc5", "RSAæ˜¯ä¸€ç§éå¯¹ç§°åŠ å¯†ç®—æ³•ï¼Œä½¿ç”¨å…¬é’¥å’Œç§é’¥è¿›è¡ŒåŠ å¯†è§£å¯†ã€‚", {"category": "crypto"}),
        ("doc6", "SSRFæœåŠ¡å™¨ç«¯è¯·æ±‚ä¼ªé€ ï¼Œæ”»å‡»è€…å¯ä»¥è®©æœåŠ¡å™¨å‘èµ·æ¶æ„è¯·æ±‚ã€‚", {"category": "web"}),
    ]
    
    print("\nğŸ“ æ·»åŠ æ–‡æ¡£...")
    for doc_id, text, metadata in documents:
        kb.add_document(doc_id, text, metadata)
        print(f"  âœ“ {doc_id}: {text[:30]}...")
    
    # è¯­ä¹‰æœç´¢
    print("\nğŸ” è¯­ä¹‰æœç´¢: 'åå…­è¿›åˆ¶'")
    results = kb.search('åå…­è¿›åˆ¶', top_k=3)
    for i, result in enumerate(results, 1):
        print(f"  {i}. [{result['doc_id']}] ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
        print(f"     æ–‡æœ¬: {result['text']}")
    
    # æ··åˆæœç´¢
    print("\nğŸ” æ··åˆæœç´¢: 'ç¼–ç æ–¹å¼'")
    results = kb.hybrid_search('ç¼–ç æ–¹å¼', top_k=3)
    for i, result in enumerate(results, 1):
        print(f"  {i}. [{result['doc_id']}] ç»„åˆåˆ†æ•°: {result['combined_score']:.3f}")
        print(f"     è¯­ä¹‰ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
        print(f"     å…³é”®è¯åˆ†æ•°: {result['keyword_score']:.3f}")
    
    # èšç±»
    print("\nğŸ“Š æ–‡æ¡£èšç±» (3ç±»)")
    clusters = kb.cluster_documents(n_clusters=3)
    for cluster_id, doc_ids in clusters.items():
        print(f"  {cluster_id}: {doc_ids}")
    
    # ç»Ÿè®¡
    print(f"\nğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡")
    stats = kb.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # ä¿å­˜
    kb._save()
    print("\nâœ… çŸ¥è¯†åº“å·²ä¿å­˜")
